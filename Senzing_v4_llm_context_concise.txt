System: This is the start of context for questions about the Senzing v4 Python SDK
# Deleting Data
The deletion snippets outline deleting previously added source records. Deleting source records removes the previously added source record from the system, completes the entity resolution process and persists outcomes in the Senzing repository.

Deleting a record only requires the data source code and record ID for the record to be deleted.

## Snippets
* **DeleteFutures.py**
    * Read and delete source records from a file using multiple threads
* **DeleteLoop.py**
    * Basic read and delete source records from a file
* **DeleteWithInfoFutures.py**
    * Read and delete source records from a file using multiple threads
    * Collect the response from the [with info](../../../README.md#with-info) version of the API and write it to a file

#! /usr/bin/env python3

import os
import sys
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


try:
    sz_factory = SzAbstractFactoryCore("add_records", SETTINGS, verbose_logging=False)
    sz_config = sz_factory.create_config()
    sz_configmanager = sz_factory.create_configmanager()

    config_id = sz_configmanager.get_default_config_id()
    config_definition = sz_configmanager.get_config(config_id)
    config_handle = sz_config.import_config(config_definition)

    for data_source in ("CUSTOMERS", "REFERENCE", "WATCHLIST"):
        response = sz_config.add_data_source(config_handle, data_source)

    config_definition = sz_config.export_config(config_handle)
    config_id = sz_configmanager.add_config(config_definition, INSTANCE_NAME)
    sz_configmanager.set_default_config_id(config_id)

    response2 = sz_config.get_data_sources(config_handle)
    sz_config.close_config(config_handle)
    print(response2)
except SzError as err:
    print(f"{err.__class__.__name__} - {err}", file=sys.stderr)
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")
INSTANCE_NAME = Path(__file__).stem

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_product = sz_factory.create_product()
    print(sz_product.get_license())
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
# Deleting Data

The information snippets outline the retrieval of different informational aspects of a Senzing instance or engine.

## Snippets

- **check_datastore_performance.py**
  - Run an insert test against the Senzing repository to gauge performance
- **get_datastore_info.py**
  - Return basic information about the Senzing repository(s)
- **get_license.py**
  - Return the currently in use license details
- **get_stats.py**
  - Return statistical information from the Senzing engine during entity resolution processing
- **get_version.py**
  - Return the current Senzing product version details
#! /usr/bin/env python3

import concurrent.futures
import itertools
import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")
INPUT_FILE = Path("../../resources/data/load-500.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def add_record(engine, record_to_add):
    record_dict = json.loads(record_to_add)
    data_source = record_dict.get("DATA_SOURCE", "")
    record_id = record_dict.get("RECORD_ID", "")
    engine.add_record(data_source, record_id, record_to_add)


def engine_stats(engine):
    try:
        print(f"\n{engine.get_stats()}\n")
    except SzRetryableError as err:
        mock_logger("WARN", err)
    except SzError as err:
        mock_logger("CRITICAL", err)
        raise err


def futures_add(engine, input_file):
    success_recs = 0
    error_recs = 0

    with open(input_file, "r", encoding="utf-8") as file:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(add_record, engine, record): record
                for record in itertools.islice(file, executor._max_workers)
            }

            while futures:
                done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                for f in done:
                    try:
                        f.result()
                    except (SzBadInputError, json.JSONDecodeError) as err:
                        mock_logger("ERROR", err, futures[f])
                        error_recs += 1
                    except SzRetryableError as err:
                        mock_logger("WARN", err, futures[f])
                        error_recs += 1
                    except (SzUnrecoverableError, SzError) as err:
                        mock_logger("CRITICAL", err, futures[f])
                        raise err
                    else:
                        record = file.readline()
                        if record:
                            futures[executor.submit(add_record, engine, record)] = record

                        success_recs += 1
                        if success_recs % 100 == 0:
                            print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)

                        if success_recs % 200 == 0:
                            engine_stats(engine)
                    finally:
                        del futures[f]

            print(f"\nSuccessfully loaded {success_recs:,} records, with" f" {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    futures_add(sz_engine, INPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")
INSTANCE_NAME = Path(__file__).stem

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_product = sz_factory.create_product()
    print(sz_product.get_version())
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")
INSTANCE_NAME = Path(__file__).stem
SECONDS_TO_RUN = 3

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_diagnostic = sz_factory.create_diagnostic()
    print(sz_diagnostic.check_datastore_performance(SECONDS_TO_RUN))
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")
INSTANCE_NAME = Path(__file__).stem

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_diagnostic = sz_factory.create_diagnostic()
    print(sz_diagnostic.get_datastore_info())
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import concurrent.futures
import json
import os
import sys
from multiprocessing import Process, Queue
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/load-500.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def add_record(engine, record_to_add):
    record_dict = json.loads(record_to_add)
    data_source = record_dict.get("DATA_SOURCE", "")
    record_id = record_dict.get("RECORD_ID", "")
    engine.add_record(data_source, record_id, record_to_add)


def producer(input_file, queue):
    with open(input_file, "r", encoding="utf-8") as file:
        for record in file:
            queue.put(record, block=True)


def consumer(engine, queue):
    success_recs = 0
    error_recs = 0

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {executor.submit(add_record, engine, queue.get()): _ for _ in range(executor._max_workers)}

        while futures:
            done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
            for f in done:
                try:
                    f.result()
                except (SzBadInputError, json.JSONDecodeError) as err:
                    mock_logger("ERROR", err, futures[f])
                    error_recs += 1
                except SzRetryableError as err:
                    mock_logger("WARN", err, futures[f])
                    error_recs += 1
                except (SzUnrecoverableError, SzError) as err:
                    mock_logger("CRITICAL", err, futures[f])
                    raise err
                else:
                    if not queue.empty():
                        record = queue.get()
                        futures[executor.submit(add_record, engine, record)] = record

                    success_recs += 1
                    if success_recs % 100 == 0:
                        print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)
                finally:
                    del futures[f]

        print(f"\nSuccessfully loaded {success_recs:,} records, with {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()

    input_queue = Queue(maxsize=200)  # type: ignore
    producer_proc = Process(target=producer, args=(INPUT_FILE, input_queue))
    producer_proc.start()
    consumer_proc = Process(target=consumer, args=(sz_engine, input_queue))
    consumer_proc.start()
    producer_proc.join()
    consumer_proc.join()

except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/load-500.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def add_records_from_file(engine, input_file):
    success_recs = 0
    error_recs = 0

    with open(input_file, "r", encoding="utf-8") as file:
        for record_to_add in file:
            try:
                record_dict = json.loads(record_to_add)
                data_source = record_dict.get("DATA_SOURCE", "")
                record_id = record_dict.get("RECORD_ID", "")
                engine.add_record(data_source, record_id, record_to_add)
            except (SzBadInputError, json.JSONDecodeError) as err:
                mock_logger("ERROR", err, record_to_add)
                error_recs += 1
            except SzRetryableError as err:
                mock_logger("WARN", err, record_to_add)
                error_recs += 1
            except (SzUnrecoverableError, SzError) as err:
                mock_logger("CRITICAL", err, record_to_add)
                raise err
            else:
                success_recs += 1

            if success_recs % 100 == 0:
                print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)

    print(f"\nSuccessfully loaded {success_recs:,} records, with {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    add_records_from_file(sz_engine, INPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
# Loading Data

The loading snippets outline adding new source records. Adding source records ingests [mapped](https://senzing.zendesk.com/hc/en-us/articles/231925448-Generic-Entity-Specification-JSON-CSV-Mapping) JSON data, completes the entity resolution process and persists outcomes in the Senzing repository. Adding a source record with the same data source code and record ID as an existing record will replace it.

## Snippets

- **add_futures.py**
  - Read and load source records from a file using multiple threads
- **add_queue.py**
  - Read and load source records using a queue
- **add_records_loop.py**
  - Basic read and add source records from a file
- **add_records.py**
  - Basic iteration over a few records, adding each one
- **add_truthset_loop.py**
  - Read and load from multiple source files, adding a sample truth set
- **add_with_info_futures.py**
  - Read and load source records from a file using multiple threads
  - Collect the response using the [SZ_WITH_INFO flag](../../README.md#with-info) on the `add_record()` method and write it to a file
#! /usr/bin/env python3

import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILES = [
    Path("../../resources/data/truthset/customers.jsonl").resolve(),
    Path("../../resources/data/truthset/reference.jsonl").resolve(),
    Path("../../resources/data/truthset/watchlist.jsonl").resolve(),
]
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def add_records_from_file(engine, input_file):
    success_recs = 0
    error_recs = 0

    with open(input_file, "r", encoding="utf-8") as file:
        print(f"\nAdding records from {input_file}")

        for record_to_add in file:
            try:
                record_dict = json.loads(record_to_add)
                data_source = record_dict.get("DATA_SOURCE", "")
                record_id = record_dict.get("RECORD_ID", "")
                engine.add_record(data_source, record_id, record_to_add)
            except (SzBadInputError, json.JSONDecodeError) as err:
                mock_logger("ERROR", err, record_to_add)
                error_recs += 1
            except SzRetryableError as err:
                mock_logger("WARN", err, record_to_add)
                error_recs += 1
            except (SzUnrecoverableError, SzError) as err:
                mock_logger("CRITICAL", err, record_to_add)
                raise err
            else:
                success_recs += 1

            if success_recs % 100 == 0:
                print(f"Processed {success_recs:,} adds, with {error_recs:,} errors")

    print(f"\nSuccessfully loaded {success_recs:,} records, with {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    for load_file in INPUT_FILES:
        add_records_from_file(sz_engine, load_file)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import json
import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
RECORDS = [
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "1001",
        "RECORD_TYPE": "PERSON",
        "PRIMARY_NAME_LAST": "Smith",
        "PRIMARY_NAME_FIRST": "Robert",
        "DATE_OF_BIRTH": "12/11/1978",
        "ADDR_TYPE": "MAILING",
        "ADDR_FULL": "123 Main Street, Las Vegas NV 89132",
        "PHONE_TYPE": "HOME",
        "PHONE_NUMBER": "702-919-1300",
        "EMAIL_ADDRESS": "bsmith@work.com",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "1002",
        "RECORD_TYPE": "PERSON",
        "PRIMARY_NAME_LAST": "Smith II",
        "PRIMARY_NAME_FIRST": "Bob",
        "DATE_OF_BIRTH": "11/12/1978",
        "ADDR_TYPE": "HOME",
        "ADDR_LINE1": "1515 Adela Lane",
        "ADDR_CITY": "Las Vegas",
        "ADDR_STATE": "NV",
        "ADDR_POSTAL_CODE": "89111",
        "PHONE_TYPE": "MOBILE",
        "PHONE_NUMBER": "702-919-1300",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "1003",
        "RECORD_TYPE": "PERSON",
        "PRIMARY_NAME_LAST": "Smith",
        "PRIMARY_NAME_FIRST": "Bob",
        "PRIMARY_NAME_MIDDLE": "J",
        "DATE_OF_BIRTH": "12/11/1978",
        "EMAIL_ADDRESS": "bsmith@work.com",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "1004",
        "RECORD_TYPE": "PERSON",
        "PRIMARY_NAME_LAST": "Smith",
        "PRIMARY_NAME_FIRST": "B",
        "ADDR_TYPE": "HOME",
        "ADDR_LINE1": "1515 Adela Ln",
        "ADDR_CITY": "Las Vegas",
        "ADDR_STATE": "NV",
        "ADDR_POSTAL_CODE": "89132",
        "EMAIL_ADDRESS": "bsmith@work.com",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "1005",
        "RECORD_TYPE": "PERSON",
        "PRIMARY_NAME_LAST": "Smith",
        "PRIMARY_NAME_FIRST": "Rob",
        "PRIMARY_NAME_MIDDLE": "E",
        "DRIVERS_LICENSE_NUMBER": "112233",
        "DRIVERS_LICENSE_STATE": "NV",
        "ADDR_TYPE": "MAILING",
        "ADDR_LINE1": "123 E Main St",
        "ADDR_CITY": "Henderson",
        "ADDR_STATE": "NV",
        "ADDR_POSTAL_CODE": "89132",
    },
]
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()

    for record in RECORDS:
        data_source = record["DATA_SOURCE"]
        record_id = record["RECORD_ID"]
        sz_engine.add_record(data_source, record_id, json.dumps(record))
        print(f"Record {record_id} added", flush=True)
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import concurrent.futures
import itertools
import json
import os
import sys
from pathlib import Path

from senzing import (
    SzBadInputError,
    SzEngineFlags,
    SzError,
    SzRetryableError,
    SzUnrecoverableError,
)
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/load-500-with-errors.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
OUTPUT_FILE = Path("../../resources/output/add_file_with_info.jsonl").resolve()
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def add_record(engine, record_to_add):
    record_dict = json.loads(record_to_add)
    data_source = record_dict.get("DATA_SOURCE", "")
    record_id = record_dict.get("RECORD_ID", "")
    return engine.add_record(data_source, record_id, record_to_add, flags=SzEngineFlags.SZ_WITH_INFO)


def engine_stats(engine):
    try:
        print(f"\n{engine.get_stats()}\n")
    except SzRetryableError as err:
        mock_logger("WARN", err)
    except SzError as err:
        mock_logger("CRITICAL", err)
        raise err


def futures_add(engine, input_file, output_file):
    success_recs = 0
    error_recs = 0

    with open(output_file, "w", encoding="utf-8") as out_file:
        with open(input_file, "r", encoding="utf-8") as in_file:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                futures = {
                    executor.submit(add_record, engine, record): record
                    for record in itertools.islice(in_file, executor._max_workers)
                }

                while futures:
                    done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                    for f in done:
                        try:
                            result = f.result()
                        except (SzBadInputError, json.JSONDecodeError) as err:
                            mock_logger("ERROR", err, futures[f])
                            error_recs += 1
                        except SzRetryableError as err:
                            mock_logger("WARN", err, futures[f])
                            error_recs += 1
                        except (SzUnrecoverableError, SzError) as err:
                            mock_logger("CRITICAL", err, futures[f])
                            raise err
                        else:
                            record = in_file.readline()
                            if record:
                                futures[executor.submit(add_record, engine, record)] = record

                            out_file.write(f"{result}\n")

                            success_recs += 1
                            if success_recs % 100 == 0:
                                print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)

                            if success_recs % 200 == 0:
                                engine_stats(engine)
                        finally:
                            del futures[f]

                print(f"\nSuccessfully loaded {success_recs:,} records, with" f" {error_recs:,} errors")
                print(f"\nWith info responses written to {output_file}")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    futures_add(sz_engine, INPUT_FILE, OUTPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import concurrent.futures
import itertools
import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/load-500.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def add_record(engine, record_to_add):
    record_dict = json.loads(record_to_add)
    data_source = record_dict.get("DATA_SOURCE", "")
    record_id = record_dict.get("RECORD_ID", "")
    engine.add_record(data_source, record_id, record_to_add)


def futures_add(engine, input_file):
    success_recs = 0
    error_recs = 0

    with open(input_file, "r", encoding="utf-8") as file:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(add_record, engine, record): record
                for record in itertools.islice(file, executor._max_workers)
            }

            while futures:
                done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                for f in done:
                    try:
                        f.result()
                    except (SzBadInputError, json.JSONDecodeError) as err:
                        mock_logger("ERROR", err, futures[f])
                        error_recs += 1
                    except SzRetryableError as err:
                        mock_logger("WARN", err, futures[f])
                        error_recs += 1
                    except (SzUnrecoverableError, SzError) as err:
                        mock_logger("CRITICAL", err, futures[f])
                        raise err
                    else:
                        record = file.readline()
                        if record:
                            futures[executor.submit(add_record, engine, record)] = record

                        success_recs += 1
                        if success_recs % 100 == 0:
                            print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)
                    finally:
                        del futures[f]

            print(f"\nSuccessfully loaded {success_recs:,} records, with" f" {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    futures_add(sz_engine, INPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import concurrent.futures
import os
import sys
import time
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def get_redo_record(engine):
    try:
        return engine.get_redo_record()
    except SzError as err:
        mock_logger("CRITICAL", err)
        raise err


def prime_redo_records(engine, quantity):
    redo_records = []
    for _ in range(quantity):
        redo_record = get_redo_record(engine)
        if redo_record:
            redo_records.append(redo_record)
    return redo_records


def process_redo_record(engine, redo_record):
    engine.process_redo_record(redo_record)


def redo_count(engine):
    try:
        return engine.count_redo_records()
    except SzRetryableError as err:
        mock_logger("WARN", err)
    except SzError as err:
        mock_logger("CRITICAL", err)
        raise err


def redo_pause(success):
    print("No redo records to process, pausing for 30 seconds. Total processed:" f" {success:,} (CTRL-C to exit)...")
    time.sleep(30)


def futures_redo(engine):
    success_recs = 0
    error_recs = 0
    redo_paused = False

    with concurrent.futures.ThreadPoolExecutor() as executor:
        while 1:
            futures = {
                executor.submit(process_redo_record, engine, record): record
                for record in prime_redo_records(engine, executor._max_workers)
            }
            if not futures:
                redo_pause(success_recs)
            else:
                break

        while 1:
            done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
            for f in done:
                try:
                    _ = f.result()
                except SzBadInputError as err:
                    mock_logger("ERROR", err, futures[f])
                    error_recs += 1
                except SzRetryableError as err:
                    mock_logger("WARN", err, futures[f])
                    error_recs += 1
                except (SzUnrecoverableError, SzError) as err:
                    mock_logger("CRITICAL", err, futures[f])
                    raise err
                else:
                    record = get_redo_record(engine)
                    if record:
                        futures[executor.submit(process_redo_record, engine, record)] = record
                    else:
                        redo_paused = True

                    success_recs += 1
                    if success_recs % 100 == 0:
                        print(f"Processed {success_recs:,} redo records, with" f" {error_recs:,} errors")
                finally:
                    del futures[f]

            if redo_paused:
                while not redo_count(engine):
                    redo_pause(success_recs)
                redo_paused = False
                while len(futures) < executor._max_workers:
                    record = get_redo_record(engine)
                    if record:
                        futures[executor.submit(process_redo_record, engine, record)] = record


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    futures_redo(sz_engine)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import os
import sys
import time
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def process_redo(engine):
    success_recs = 0
    error_recs = 0

    while 1:
        try:
            response = engine.get_redo_record()

            if not response:
                print(
                    "No redo records to process, pausing for 30 seconds. Total"
                    f" processed {success_recs:,} . (CTRL-C to exit)..."
                )
                time.sleep(30)
                continue

            engine.process_redo_record(response)

            success_recs += 1
            if success_recs % 100 == 0:
                print(f"Processed {success_recs:,} redo records, with" f" {error_recs:,} errors")
        except SzBadInputError as err:
            mock_logger("ERROR", err)
            error_recs += 1
        except SzRetryableError as err:
            mock_logger("WARN", err)
            error_recs += 1
        except (SzUnrecoverableError, SzError) as err:
            mock_logger("CRITICAL", err)
            raise err


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    process_redo(sz_engine)
except SzError as err:
    mock_logger("CRITICAL", err)
# Redo Records

The redo snippets outline processing redo records. During normal processing of loading, deleting and replacing data the Senzing engine may determine additional work needs to be completed for an entity. There are times the Senzing engine will decide to defer this additional work. Examples of why this may happen include:

- Records loaded in parallel are clustering around the same entities causing contention
- Automatic corrections
- Cleansing decisions made on attributes determined to no longer be useful for entity resolution

When an entity requires additional work a record is automatically created in the system indicating this requirement. These records are called redo records. Redo records need to be periodically or continuously checked for and processed. Periodically is suitable after manipulating smaller portions of data, for example, at the end of a batch load of data. In contrast, a continuous process checking for and processing redo records is suitable in a streaming system that is constantly manipulating data. In general, it is recommended to have a continuous redo process checking for any redo records to process and processing them.

## Snippets

- **add_with_redo.py**
  - Read and load source records from a file and then process any redo records
- **redo_continuous_futures.py**
  - Continuously monitor for redo records to process using multiple threads
- **redo_continuous.py**
  - Basic example of continuously monitoring for redo records to process
- **redo_with_info_continuous.py**
  - Continuously monitor for redo records to process
  - Collect the response using the [SZ_WITH_INFO flag](../../README.md#with-info) on the `process_redo_record()` method and write it to a file
#! /usr/bin/env python3

import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILES = [
    Path("../../resources/data/truthset/customers.jsonl").resolve(),
    Path("../../resources/data/truthset/reference.jsonl").resolve(),
    Path("../../resources/data/truthset/watchlist.jsonl").resolve(),
]
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def add_records_from_file(engine, input_file):
    success_recs = 0
    error_recs = 0

    with open(input_file, "r", encoding="utf-8") as file:

        for record_to_add in file:
            try:
                record_dict = json.loads(record_to_add)
                data_source = record_dict.get("DATA_SOURCE", None)
                record_id = record_dict.get("RECORD_ID", None)
                engine.add_record(data_source, record_id, record_to_add)
            except (SzBadInputError, json.JSONDecodeError) as err:
                mock_logger("ERROR", err, record_to_add)
                error_recs += 1
            except SzRetryableError as err:
                mock_logger("WARN", err, record_to_add)
                error_recs += 1
            except (SzUnrecoverableError, SzError) as err:
                mock_logger("CRITICAL", err, record_to_add)
                raise err
            else:
                success_recs += 1

            if success_recs % 100 == 0:
                print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)

    print(f"\nSuccessfully added {success_recs:,} records, with {error_recs:,} errors")


def process_redo(engine):
    success_recs = 0
    error_recs = 0

    print("\nStarting to process redo records...")

    while 1:
        try:
            response = engine.get_redo_record()
            if not response:
                break
            engine.process_redo_record(response)

            success_recs += 1
            if success_recs % 1 == 0:
                print(f"Processed {success_recs:,} redo records, with" f" {error_recs:,} errors")
        except SzBadInputError as err:
            mock_logger("ERROR", err)
            error_recs += 1
        except SzRetryableError as err:
            mock_logger("WARN", err)
            error_recs += 1
        except (SzUnrecoverableError, SzError) as err:
            mock_logger("CRITICAL", err)
            raise err

    print(f"\nSuccessfully processed {success_recs:,} redo records, with" f" {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    for load_file in INPUT_FILES:
        add_records_from_file(sz_engine, load_file)
    redo_count = sz_engine.count_redo_records()

    if redo_count:
        process_redo(sz_engine)
    else:
        print("\nNo redo records to process")
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import os
import signal
import sys
import time
from pathlib import Path

from senzing import (
    SzBadInputError,
    SzEngineFlags,
    SzError,
    SzRetryableError,
    SzUnrecoverableError,
)
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
OUTPUT_FILE = Path("../../resources/output/redo_with_info_continuous.jsonl").resolve()
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def signal_handler(signum, frame):
    print(f"\nWith info responses written to {OUTPUT_FILE}")
    sys.exit()


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def redo_pause(success):
    print("No redo records to process, pausing for 30 seconds. Total processed:" f" {success:,} (CTRL-C to exit)...")
    time.sleep(30)


def process_redo(engine, output_file):
    success_recs = 0
    error_recs = 0

    with open(output_file, "w", encoding="utf-8") as out_file:
        try:
            while 1:
                redo_record = engine.get_redo_record()

                if not redo_record:
                    redo_pause(success_recs)
                    continue

                response = engine.process_redo_record(redo_record, flags=SzEngineFlags.SZ_WITH_INFO)
                success_recs += 1
                out_file.write(f"{response}\n")

                if success_recs % 100 == 0:
                    print(f"Processed {success_recs:,} redo records, with" f" {error_recs:,} errors")
        except SzBadInputError as err:
            mock_logger("ERROR", err, redo_record)
            error_recs += 1
        except SzRetryableError as err:
            mock_logger("WARN", err, redo_record)
            error_recs += 1
        except (SzUnrecoverableError, SzError) as err:
            mock_logger("CRITICAL", err, redo_record)
            raise err


signal.signal(signal.SIGINT, signal_handler)

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    process_redo(sz_engine, OUTPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
# Searching for Entities

The search snippets outline searching for entities in the system. Searching for entities uses the same mapped JSON data [specification](https://senzing.zendesk.com/hc/en-us/articles/231925448-Generic-Entity-Specification-JSON-CSV-Mapping) as SDK methods such as `add_record()` to format the search request.

There are [considerations](https://senzing.zendesk.com/hc/en-us/articles/360007880814-Guidelines-for-Successful-Entity-Searching) to be aware of when searching.

## Snippets

- **search_futures.py**
  - Read and search for records from a file using multiple threads
  - To see results first load records with [add_futures.py](../loading/add_futures.py)
- **search_records.py**
  - Basic iteration over a few records, searching for each one
  - To see results first load records with [add_truthset_loop.py](../loading/add_truthset_loop.py)
#! /usr/bin/env python3

import concurrent.futures
import itertools
import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/search-50.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def search_record(engine, record_to_search):
    return engine.search_by_attributes(record_to_search)


def futures_search(engine, input_file):
    success_recs = 0
    error_recs = 0

    with open(input_file, "r", encoding="utf-8") as in_file:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(search_record, engine, record): record
                for record in itertools.islice(in_file, executor._max_workers)
            }

            while futures:
                done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                for f in done:
                    try:
                        result = f.result()
                    except (SzBadInputError, json.JSONDecodeError) as err:
                        mock_logger("ERROR", err, futures[f])
                        error_recs += 1
                    except SzRetryableError as err:
                        mock_logger("WARN", err, futures[f])
                        error_recs += 1
                    except (SzUnrecoverableError, SzError) as err:
                        mock_logger("CRITICAL", err, futures[f])
                        raise err
                    else:
                        record = in_file.readline()
                        if record:
                            futures[executor.submit(search_record, engine, record)] = record

                        success_recs += 1
                        if success_recs % 100 == 0:
                            print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)

                        print(f"\n------ Searched: {futures[f]}", flush=True)
                        print(f"\n{result}", flush=True)
                    finally:
                        del futures[f]

            print(f"\nSuccessfully searched {success_recs:,} records, with" f" {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    futures_search(sz_engine, INPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")

search_records = [
    {
        "NAME_FULL": "Susan Moony",
        "DATE_OF_BIRTH": "15/6/1998",
        "SSN_NUMBER": "521212123",
    },
    {
        "NAME_FIRST": "Robert",
        "NAME_LAST": "Smith",
        "ADDR_FULL": "123 Main Street Las Vegas NV 89132",
    },
    {
        "NAME_FIRST": "Makio",
        "NAME_LAST": "Yamanaka",
        "ADDR_FULL": "787 Rotary Drive Rotorville FL 78720",
    },
]


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def searcher(engine):
    for search_record in search_records:
        try:
            record_str = json.dumps(search_record)
            response = engine.search_by_attributes(record_str)
        except (SzBadInputError, json.JSONDecodeError) as err:
            mock_logger("ERROR", err, record_str)
        except SzRetryableError as err:
            mock_logger("WARN", err, record_str)
        except (SzUnrecoverableError, SzError) as err:
            mock_logger("CRITICAL", err, record_str)
            raise err

        print(f"\n------ Searched: {record_str}", flush=True)
        print(f"\n{response}", flush=True)


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    searcher(sz_engine)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import os
import sys
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
PURGE_MSG = """
**************************************** WARNING ****************************************

This example will purge all currently loaded data from the Senzing datastore!
Before proceeding, all instances of Senzing (custom code, tools, etc.) must be shut down.

*****************************************************************************************

Are you sure you want to continue and purge the Senzing datastore? Type YESPURGESENZING to purge: """
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


if input(PURGE_MSG) != "YESPURGESENZING":
    sys.exit()

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_diagnostic = sz_factory.create_diagnostic()
    sz_diagnostic.purge_repository()
    print("\nSenzing datastore purged")
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

# The value of config_id is made up, this example will fail if you run it
CONFIG_ID = 2787481550
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")

try:
    sz_abstract_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, CONFIG_ID)
    sz_abstract_factory.create_engine()
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_config = sz_factory.create_config()
    sz_configmgr = sz_factory.create_configmanager()
    sz_diagnostic = sz_factory.create_diagnostic()
    sz_engine = sz_factory.create_engine()
    sz_product = sz_factory.create_product()
    # Do work...
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import configparser
from pathlib import Path

INI_FILE = Path("../../resources/g2module/G2Module.ini").resolve()
settings = {}

cfgp = configparser.ConfigParser()
cfgp.optionxform = str  # type: ignore
cfgp.read(INI_FILE)

for section in cfgp.sections():
    settings[section] = dict(cfgp.items(section))

print(settings)
# Initialization

## Snippets

- **abstract_factory_parameters.py**
  - Used to create a dictionary that can be unpacked when creating an SzAbstractFactoryCore, also useful for type annotations
- **engine_priming.py**
  - Priming the Senzing engine before use loads resource intensive assets upfront. Without priming the first SDK call to the engine will appear slower than usual as it causes these assets to be loaded
- **factory_and_engines.py**
  - Basic example of how to create an abstract Senzing factory and each of the available engines
- **g2_module_ini_to_json.py**
  - The snippets herein utilize the `SENZING_ENGINE_CONFIGURATION_JSON` environment variable for Senzing abstract factory creation
  - If you are familiar with working with a Senzing project you may be aware the same configuration data is held in the G2Module.ini file
  - Example to convert G2Module.ini to a JSON string for use with `SENZING_ENGINE_CONFIGURATION_JSON`
- **purge_repository.py**
  - **WARNING** This script will remove all data from a Senzing repository, use with caution! **WARNING**
  - It will prompt first, still use with caution!
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    print("Priming Senzing engine...")
    sz_engine.prime_engine()
    # Do work...
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3

import os
from pathlib import Path

from senzing import SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")
VERBOSE_LOGGING = 1

try:
    sz_abstract_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=VERBOSE_LOGGING)
    # Create an engine to show debug output
    sz_abstract_factory.create_engine()
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
#! /usr/bin/env python3


from senzing import SzError
from senzing_core import SzAbstractFactoryCore, SzAbstractFactoryParametersCore

FACTORY_PARAMETERS: SzAbstractFactoryParametersCore = {
    "instance_name": "abstract_factory_parameters",
    "settings": {
        "PIPELINE": {
            "CONFIGPATH": "/etc/opt/senzing",
            "RESOURCEPATH": "/opt/senzing/er/resources",
            "SUPPORTPATH": "/opt/senzing/data",
        },
        "SQL": {"CONNECTION": "sqlite3://na:na@/tmp/sqlite/G2C.db"},
    },
    "verbose_logging": 0,
}

try:
    sz_factory = SzAbstractFactoryCore(**FACTORY_PARAMETERS)
    sz_config = sz_factory.create_config()
    sz_configmgr = sz_factory.create_configmanager()
    sz_diagnostic = sz_factory.create_diagnostic()
    sz_engine = sz_factory.create_engine()
    sz_product = sz_factory.create_product()
    # Do work...
except SzError as err:
    print(f"\n{err.__class__.__name__} - {err}")
# Python Snippets
#! /usr/bin/env python3

import json
import os
import sys
from pathlib import Path

from senzing import SzEngineFlags, SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
PURGE_MSG = """
**************************************** WARNING ****************************************

This example will purge all currently loaded data from the Senzing datastore!
Before proceeding, all instances of Senzing (custom code, tools, etc.) must be shut down.

*****************************************************************************************

Are you sure you want to continue and purge the Senzing datastore? Type YESPURGESENZING to purge: """
RECORDS = [
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "4",
        "PRIMARY_NAME_FULL": "Elizabeth Jonas",
        "ADDR_FULL": "202 Rotary Dr, Rotorville, RI, 78720",
        "SSN_NUMBER": "767-87-7678",
        "DATE_OF_BIRTH": "1/12/1990",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "5",
        "PRIMARY_NAME_FULL": "Beth Jones",
        "ADDR_FULL": "202 Rotary Dr, Rotorville, RI, 78720",
        "SSN_NUMBER": "767-87-7678",
        "DATE_OF_BIRTH": "1/12/1990",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "6",
        "PRIMARY_NAME_FULL": "Betsey Jones",
        "ADDR_FULL": "202 Rotary Dr, Rotorville, RI, 78720",
        "PHONE_NUMBER": "202-787-7678",
    },
]
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")

if input(PURGE_MSG) != "YESPURGESENZING":
    sys.exit()

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_diagnostic = sz_factory.create_diagnostic()
    sz_engine = sz_factory.create_engine()
    sz_diagnostic.purge_repository()

    print()
    for record in RECORDS:
        data_source = record["DATA_SOURCE"]
        record_id = record["RECORD_ID"]
        sz_engine.add_record(data_source, record_id, json.dumps(record))
        print(f"Record {record_id} added")

    print()
    for record_id in ("4", "5", "6"):
        response1 = sz_engine.get_entity_by_record_id(
            "TEST",
            record_id,
            SzEngineFlags.SZ_ENTITY_BRIEF_DEFAULT_FLAGS,
        )
        get_json = json.loads(response1)
        print(f"Record {record_id} currently resolves to entity" f" {get_json['RESOLVED_ENTITY']['ENTITY_ID']}")

    print("\nUpdating records with TRUSTED_ID to force unresolve...\n")
    record1 = sz_engine.get_record("TEST", "4")
    record2 = sz_engine.get_record("TEST", "6")
    get1_json = json.loads(record1)
    get2_json = json.loads(record2)
    get1_json["JSON_DATA"].update({"TRUSTED_ID_NUMBER": "TEST_R4-TEST_R6", "TRUSTED_ID_TYPE": "FORCE_UNRESOLVE"})
    get2_json["JSON_DATA"].update({"TRUSTED_ID_NUMBER": "TEST_R6-TEST_R4", "TRUSTED_ID_TYPE": "FORCE_UNRESOLVE"})
    sz_engine.add_record("TEST", "4", json.dumps(get1_json["JSON_DATA"]))
    sz_engine.add_record("TEST", "6", json.dumps(get2_json["JSON_DATA"]))

    for record_id in ("4", "5", "6"):
        response2 = sz_engine.get_entity_by_record_id(
            "TEST",
            record_id,
            SzEngineFlags.SZ_ENTITY_BRIEF_DEFAULT_FLAGS,
        )
        get_json = json.loads(response2)
        print(f"Record {record_id} now resolves to entity" f" {get_json['RESOLVED_ENTITY']['ENTITY_ID']}")
except SzError as err:
    print(f"{err.__class__.__name__} - {err}", file=sys.stderr)
# Stewardship

The stewardship snippets outline forced resolution and un-resolution of records from entities. Stewardship provides the ability to force records to resolve or un-resolve when, for example, Senzing doesn't have enough information at a point in time, but you may have knowledge outside of Senzing to override a decision Senzing has made. Basic stewardship utilizes the `TRUSTED_ID` feature to influence entity resolution. See the [Entity Specification](https://senzing.zendesk.com/hc/en-us/articles/231925448-Generic-Entity-Specification-JSON-CSV-Mapping) for additional details.

In these examples, the current JSON data for a record is first retrieved and additional `TRUSTED_ID` attributes are appended before replacing the records and completing entity resolution, now taking into account the influence of the `TRUSTED_ID` attributes:

- TRUSTED_ID_NUMBER - when the values across records is the same the records resolve to the same entity. If the values used across records differ, the records will not resolve to the same entity.
- TRUSTED_ID_TYPE - an arbitrary value to indicate the use of the TRUSTED_ID_NUMBER.

## Snippets

- **force_resolve.py**
  - Force resolve records together to a single entity
- **force_unresolve.py**
  - Force un-resolve a record from an entity into a new entity

## Example Usage

### Force Resolve

Force resolve first adds 3 records and details which entity they each belong to.

With additional knowledge not represented in Senzing you know record 3 "Pat Smith" represents the same person as record 1 "Patrick Smith". To force resolve these 2 records to the same entity, first fetch the current representation of each record with `get_record()`. Next add `TRUSTED_ID_NUMBER` and `TRUSTED_ID_TYPE` attributes to each of the retrieved records. `TRUSTED_ID_NUMBER` uses the same value to indicate these records should always be considered the same entity and resolve together. In this example the data source of the records and their record IDs are used to create `TRUSTED_ID_NUMBER`. `TRUSTED_ID_TYPE` is set as FORCE_RESOLVE as an indicator they were forced together.

### Force UnResolve

Force UnResolve first adds 3 records and details all records resolved to the same entity.

With additional knowledge not represented in Senzing you know record 6 "Betsey Jones" is not the same as records 4 and 5; Betsey is a twin to "Elizabeth Jones". To force unresolve "Betsey" from the "Elizabeth" entity, first fetch the current representation of each record with `get_record()`. Next add `TRUSTED_ID_NUMBER` and `TRUSTED_ID_TYPE` attributes to each of the retrieved records. `TRUSTED_ID_NUMBER` uses a different value to indicate these records should always be considered different entities and not resolve together. In this example the data source of the records and their record IDs are used to create `TRUSTED_ID_NUMBER`. `TRUSTED_ID_TYPE` is set as FORCE_UNRESOLVE as an indicator they were forced apart.
#! /usr/bin/env python3

import json
import os
import sys
from pathlib import Path

from senzing import SzEngineFlags, SzError
from senzing_core import SzAbstractFactoryCore

INSTANCE_NAME = Path(__file__).stem
PURGE_MSG = """
**************************************** WARNING ****************************************

This example will purge all currently loaded data from the Senzing datastore!
Before proceeding, all instances of Senzing (custom code, tools, etc.) must be shut down.

*****************************************************************************************

Are you sure you want to continue and purge the Senzing datastore? Type YESPURGESENZING to purge: """
RECORDS = [
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "1",
        "PRIMARY_NAME_FULL": "Patrick Smith",
        "AKA_NAME_FULL": "Paddy Smith",
        "ADDR_FULL": "787 Rotary Dr, Rotorville, RI, 78720",
        "PHONE_NUMBER": "787-767-2688",
        "DATE_OF_BIRTH": "1/12/1990",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "2",
        "PRIMARY_NAME_FULL": "Patricia Smith",
        "ADDR_FULL": "787 Rotary Dr, Rotorville, RI, 78720",
        "PHONE_NUMBER": "787-767-2688",
        "DATE_OF_BIRTH": "5/4/1994",
    },
    {
        "DATA_SOURCE": "TEST",
        "RECORD_ID": "3",
        "PRIMARY_NAME_FULL": "Pat Smith",
        "ADDR_FULL": "787 Rotary Dr, Rotorville, RI, 78720",
        "PHONE_NUMBER": "787-767-2688",
    },
]
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


if input(PURGE_MSG) != "YESPURGESENZING":
    sys.exit()

try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_diagnostic = sz_factory.create_diagnostic()
    sz_engine = sz_factory.create_engine()
    sz_diagnostic.purge_repository()

    print()
    for record in RECORDS:
        data_source = record["DATA_SOURCE"]
        record_id = record["RECORD_ID"]
        sz_engine.add_record(data_source, record_id, json.dumps(record))
        print(f"Record {record_id} added")

    print()
    for record_id in ("1", "2", "3"):
        response1 = sz_engine.get_entity_by_record_id(
            "TEST",
            record_id,
            SzEngineFlags.SZ_ENTITY_BRIEF_DEFAULT_FLAGS,
        )
        get_json = json.loads(response1)
        print(f"Record {record_id} currently resolves to entity" f" {get_json['RESOLVED_ENTITY']['ENTITY_ID']}")

    print("\nUpdating records with TRUSTED_ID to force resolve...\n")
    record1 = sz_engine.get_record("TEST", "1")
    record2 = sz_engine.get_record("TEST", "3")
    get1_json = json.loads(record1)
    get2_json = json.loads(record2)
    get1_json["JSON_DATA"].update({"TRUSTED_ID_NUMBER": "TEST_R1-TEST_R3", "TRUSTED_ID_TYPE": "FORCE_RESOLVE"})
    get2_json["JSON_DATA"].update({"TRUSTED_ID_NUMBER": "TEST_R1-TEST_R3", "TRUSTED_ID_TYPE": "FORCE_RESOLVE"})
    sz_engine.add_record("TEST", "1", json.dumps(get1_json["JSON_DATA"]))
    sz_engine.add_record("TEST", "3", json.dumps(get2_json["JSON_DATA"]))

    for record_id in ("1", "2", "3"):
        response2 = sz_engine.get_entity_by_record_id(
            "TEST",
            record_id,
            SzEngineFlags.SZ_ENTITY_BRIEF_DEFAULT_FLAGS,
        )
        get_json = json.loads(response2)
        print(f"Record {record_id} now resolves to entity" f" {get_json['RESOLVED_ENTITY']['ENTITY_ID']}")
except SzError as err:
    print(f"{err.__class__.__name__} - {err}", file=sys.stderr)
# Replacing Data

Adding a source record with the same data source code and record ID as an existing record will replace it. See the snippets in the loading path for examples on how to add records.
#! /usr/bin/env python3

import concurrent.futures
import itertools
import json
import os
import sys
from pathlib import Path

from senzing import (
    SzBadInputError,
    SzEngineFlags,
    SzError,
    SzRetryableError,
    SzUnrecoverableError,
)
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/del-500.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
OUTPUT_FILE = Path("../../resources/output/delete_file_with_info.jsonl").resolve()
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def delete_record(engine, record_to_delete):
    record_dict = json.loads(record_to_delete)
    data_source = record_dict.get("DATA_SOURCE", "")
    record_id = record_dict.get("RECORD_ID", "")
    return engine.delete_record(data_source, record_id, flags=SzEngineFlags.SZ_WITH_INFO)


def futures_del(engine, input_file, output_file):
    success_recs = 0
    error_recs = 0

    with open(output_file, "w", encoding="utf-8") as out_file:
        with open(input_file, "r", encoding="utf-8") as in_file:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                futures = {
                    executor.submit(delete_record, engine, record): record
                    for record in itertools.islice(in_file, executor._max_workers)
                }

                while futures:
                    done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                    for f in done:
                        try:
                            result = f.result()
                        except (SzBadInputError, json.JSONDecodeError) as err:
                            mock_logger("ERROR", err, futures[f])
                            error_recs += 1
                        except SzRetryableError as err:
                            mock_logger("WARN", err, futures[f])
                            error_recs += 1
                        except (SzUnrecoverableError, SzError) as err:
                            mock_logger("CRITICAL", err, futures[f])
                            raise err
                        else:
                            record = in_file.readline()
                            if record:
                                futures[executor.submit(delete_record, engine, record)] = record

                            out_file.write(f"{result}\n")

                            success_recs += 1
                            if success_recs % 100 == 0:
                                print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)
                        finally:
                            del futures[f]

                print(f"\nSuccessfully deleted {success_recs:,} records, with" f" {error_recs:,} errors")
                print(f"\nWith info responses written to {output_file}")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    futures_del(sz_engine, INPUT_FILE, OUTPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/del-500.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def del_records_from_file(engine, input_file):
    success_recs = error_recs = 0

    with open(input_file, "r", encoding="utf-8") as file:

        for record_to_delete in file:
            try:
                record_dict = json.loads(record_to_delete)
                data_source = record_dict.get("DATA_SOURCE", "")
                record_id = record_dict.get("RECORD_ID", "")
                engine.delete_record(data_source, record_id)
            except (SzBadInputError, json.JSONDecodeError) as err:
                mock_logger("ERROR", err, record_to_delete)
                error_recs += 1
            except SzRetryableError as err:
                mock_logger("WARN", err, record_to_delete)
                error_recs += 1
            except (SzUnrecoverableError, SzError) as err:
                mock_logger("CRITICAL", err, record_to_delete)
                raise err
            else:
                success_recs += 1

            if success_recs % 100 == 0:
                print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)

    print(f"\nSuccessfully deleted {success_recs:,} records, with {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    del_records_from_file(sz_engine, INPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
#! /usr/bin/env python3

import concurrent.futures
import itertools
import json
import os
import sys
from pathlib import Path

from senzing import SzBadInputError, SzError, SzRetryableError, SzUnrecoverableError
from senzing_core import SzAbstractFactoryCore

INPUT_FILE = Path("../../resources/data/del-500.jsonl").resolve()
INSTANCE_NAME = Path(__file__).stem
SETTINGS = os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", "{}")


def mock_logger(level, error, error_record=None):
    print(f"\n{level}: {error.__class__.__name__} - {error}", file=sys.stderr)
    if error_record:
        print(f"{error_record}", file=sys.stderr)


def delete_record(engine, record_to_delete):
    record_dict = json.loads(record_to_delete)
    data_source = record_dict.get("DATA_SOURCE", "")
    record_id = record_dict.get("RECORD_ID", "")
    engine.delete_record(data_source, record_id)


def futures_del(engine, input_file):
    success_recs = 0
    error_recs = 0

    with open(input_file, "r", encoding="utf-8") as in_file:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(delete_record, engine, record): record
                for record in itertools.islice(in_file, executor._max_workers)
            }

            while futures:
                done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                for f in done:
                    try:
                        f.result()
                    except (SzBadInputError, json.JSONDecodeError) as err:
                        mock_logger("ERROR", err, futures[f])
                        error_recs += 1
                    except SzRetryableError as err:
                        mock_logger("WARN", err, futures[f])
                        error_recs += 1
                    except (SzUnrecoverableError, SzError) as err:
                        mock_logger("CRITICAL", err, futures[f])
                        raise err
                    else:
                        record = in_file.readline()
                        if record:
                            futures[executor.submit(delete_record, engine, record)] = record

                        success_recs += 1
                        if success_recs % 100 == 0:
                            print(f"Processed {success_recs:,} adds, with {error_recs:,} errors", flush=True)
                    finally:
                        del futures[f]

            print(f"\nSuccessfully deleted {success_recs:,} records, with" f" {error_recs:,} errors")


try:
    sz_factory = SzAbstractFactoryCore(INSTANCE_NAME, SETTINGS, verbose_logging=False)
    sz_engine = sz_factory.create_engine()
    futures_del(sz_engine, INPUT_FILE)
except SzError as err:
    mock_logger("CRITICAL", err)
# Deleting Data

The deletion snippets outline deleting previously added source records. Deleting source records removes the previously added source record from the system, completes the entity resolution process and persists outcomes in the Senzing repository.

Deleting a record only requires the data source code and record ID for the record to be deleted.

## Snippets

- **delete_futures.py**
  - Read and delete source records from a file using multiple threads
- **delete_loop.py**
  - Basic read and delete source records from a file
- **delete_with_info_futures.py**
  - Read and delete source records from a file using multiple threads
  - Collect the response using the [SZ_WITH_INFO flag](../../README.md#with-info) on the `delete_record()` method and write it to a file


System: This is the end of context for questions about the Senzing v4 Python SDK
